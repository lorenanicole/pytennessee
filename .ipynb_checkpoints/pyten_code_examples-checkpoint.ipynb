{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): www.ctabustracker.com\n",
      "2016-02-07 08:02:23,511 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): www.ctabustracker.com\n",
      "DEBUG:requests.packages.urllib3.connectionpool:\"GET /bustime/api/v1/getpredictions?key=T6ketWk5cWetPgYVqkvEVJVng&rt=72&stpid=890,944&top=3 HTTP/1.1\" 200 1065\n",
      "2016-02-07 08:02:24,061 requests.packages.urllib3.connectionpool DEBUG    \"GET /bustime/api/v1/getpredictions?key=T6ketWk5cWetPgYVqkvEVJVng&rt=72&stpid=890,944&top=3 HTTP/1.1\" 200 1065\n",
      "INFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): www.ctabustracker.com\n",
      "2016-02-07 08:02:24,066 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): www.ctabustracker.com\n",
      "DEBUG:requests.packages.urllib3.connectionpool:\"GET /bustime/api/v1/getpredictions?key=T6ketWk5cWetPgYVqkvEVJVng&rt=52&stpid=3183,3006&top=3 HTTP/1.1\" 200 1098\n",
      "2016-02-07 08:02:24,206 requests.packages.urllib3.connectionpool DEBUG    \"GET /bustime/api/v1/getpredictions?key=T6ketWk5cWetPgYVqkvEVJVng&rt=52&stpid=3183,3006&top=3 HTTP/1.1\" 200 1098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"distance_to_stop\": 18023, \n",
      "    \"route\": \"52\", \n",
      "    \"vehicle_id\": \"1807\", \n",
      "    \"arrival_time\": \"20160207 08:18\", \n",
      "    \"requested_time\": \"20160207 08:01\", \n",
      "    \"bus_eta\": 17.0, \n",
      "    \"route_direction\": \"Northbound\", \n",
      "    \"stop_name\": \"California & Le Moyne\"\n",
      "}\n",
      "{\n",
      "    \"distance_to_stop\": 13524, \n",
      "    \"route\": \"52\", \n",
      "    \"vehicle_id\": \"1800\", \n",
      "    \"arrival_time\": \"20160207 08:19\", \n",
      "    \"requested_time\": \"20160207 08:01\", \n",
      "    \"bus_eta\": 18.0, \n",
      "    \"route_direction\": \"Southbound\", \n",
      "    \"stop_name\": \"California & Milwaukee (Blue Line)\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/usr/local/lib/python2.7/site-packages')\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "from cta_api import CTABustracker, CTATraintracker\n",
    "from serializers import MyEncoder\n",
    "\n",
    "bus_tracker = CTABustracker()\n",
    "north_predictions = bus_tracker.get_predictions_for_stops(route_id=72, stp_id=\"890,944\")  # North Ave Bus Predictions\n",
    "cali_predictions = bus_tracker.get_predictions_for_stops(route_id=52, stp_id=\"3183,3006\")  # California Bus Predictions\n",
    "\n",
    "for prediction in north_predictions:\n",
    "    print json.dumps(prediction, cls=MyEncoder, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): www.ctabustracker.com\n",
      "2016-02-07 08:06:13,476 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): www.ctabustracker.com\n",
      "DEBUG:requests.packages.urllib3.connectionpool:\"GET /bustime/api/v1/getpredictions?key=T6ketWk5cWetPgYVqkvEVJVng&rt=72&stpid=890,944&top=3 HTTP/1.1\" 200 1551\n",
      "2016-02-07 08:06:13,638 requests.packages.urllib3.connectionpool DEBUG    \"GET /bustime/api/v1/getpredictions?key=T6ketWk5cWetPgYVqkvEVJVng&rt=72&stpid=890,944&top=3 HTTP/1.1\" 200 1551\n",
      "INFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): www.ctabustracker.com\n",
      "2016-02-07 08:06:13,643 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): www.ctabustracker.com\n",
      "DEBUG:requests.packages.urllib3.connectionpool:\"GET /bustime/api/v1/getpredictions?key=T6ketWk5cWetPgYVqkvEVJVng&rt=52&stpid=3183,3006&top=3 HTTP/1.1\" 200 1594\n",
      "2016-02-07 08:06:13,804 requests.packages.urllib3.connectionpool DEBUG    \"GET /bustime/api/v1/getpredictions?key=T6ketWk5cWetPgYVqkvEVJVng&rt=52&stpid=3183,3006&top=3 HTTP/1.1\" 200 1594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"tmstmp\": \"20160207 08:05\", \n",
      "    \"typ\": \"A\", \n",
      "    \"stpnm\": \"North Ave & California\", \n",
      "    \"stpid\": \"890\", \n",
      "    \"vid\": \"8130\", \n",
      "    \"dstp\": \"7674\", \n",
      "    \"rt\": \"72\", \n",
      "    \"rtdir\": \"Eastbound\", \n",
      "    \"des\": \"Clark\", \n",
      "    \"prdtm\": \"20160207 08:14\", \n",
      "    \"tablockid\": \"72 -802\", \n",
      "    \"tatripid\": \"1009981\", \n",
      "    \"zone\": null\n",
      "}\n",
      "{\n",
      "    \"tmstmp\": \"20160207 08:06\", \n",
      "    \"typ\": \"A\", \n",
      "    \"stpnm\": \"North Ave & Damen\", \n",
      "    \"stpid\": \"944\", \n",
      "    \"vid\": \"1055\", \n",
      "    \"dstp\": \"9757\", \n",
      "    \"rt\": \"72\", \n",
      "    \"rtdir\": \"Westbound\", \n",
      "    \"des\": \"Harlem\", \n",
      "    \"prdtm\": \"20160207 08:16\", \n",
      "    \"tablockid\": \"72 -801\", \n",
      "    \"tatripid\": \"1010053\", \n",
      "    \"zone\": null\n",
      "}\n",
      "{\n",
      "    \"tmstmp\": \"20160207 08:04\", \n",
      "    \"typ\": \"A\", \n",
      "    \"stpnm\": \"North Ave & California\", \n",
      "    \"stpid\": \"890\", \n",
      "    \"vid\": \"1934\", \n",
      "    \"dstp\": \"29138\", \n",
      "    \"rt\": \"72\", \n",
      "    \"rtdir\": \"Eastbound\", \n",
      "    \"des\": \"Clark\", \n",
      "    \"prdtm\": \"20160207 08:33\", \n",
      "    \"tablockid\": \"72 -806\", \n",
      "    \"tatripid\": \"1009982\", \n",
      "    \"zone\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/usr/local/lib/python2.7/site-packages')\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "from cta_api import CTABustracker, CTATraintracker\n",
    "from serializers import MyEncoder\n",
    "\n",
    "bus_tracker = CTABustracker()\n",
    "north_predictions = bus_tracker.get_raw_predictions_for_stops(route_id=72, stp_id=\"890,944\")  # North Ave Bus Predictions\n",
    "cali_predictions = bus_tracker.get_raw_predictions_for_stops(route_id=52, stp_id=\"3183,3006\")  # California Bus Predictions\n",
    "\n",
    "for prediction in north_predictions:\n",
    "    print json.dumps(prediction, cls=MyEncoder, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/usr/local/lib/python2.7/site-packages')\n",
    "\n",
    "import json\n",
    "from serializers import BusPrediction\n",
    "import pylab\n",
    "import pandas as pd\n",
    "# from cta_api import CTABustracker\n",
    "\n",
    "# Can live query for data:\n",
    "# bus_tracker = CTABustracker()\n",
    "# north_predictions = bus_tracker.get_predictions_for_stops(route_id=72, stp_id=\"890\")  # North Ave Bus Predictions\n",
    "# north_ave_etas = [bus_eta for bus_eta in bus_data if 6 < bus_eta.get_requested_time_hour() < 2 and bus_eta.stop_name == 'North Ave & California']\n",
    "# north_ave_etas = [float(bus.bus_eta) for bus in north_ave_etas]\n",
    "\n",
    "df = pd.read_csv(\"/Users/lorenamesa/Desktop/pytennessee/bus_data.csv\", index_col = [0, 1])\n",
    "# print type(df)\n",
    "\n",
    "north_ave = [row.to_dict() for index, row in df.iterrows() if row[-1] == 'North Ave & California']\n",
    "for indx, bus in enumerate(north_ave):\n",
    "    clean_d = { k.replace(\"'\",\"\").strip(): v for k, v in bus.iteritems() }\n",
    "    north_ave[indx] = BusPrediction(**clean_d)\n",
    "    \n",
    "morning_north_ave = [float(prediction.bus_eta) for prediction in north_ave if 6 < prediction.get_requested_time_hour() < 12]\n",
    "\n",
    "pylab.figure()\n",
    "data = pylab.hist(morning_north_ave, bins=6)\n",
    "\n",
    "pylab.title(\"North (#72) Bus ETAs From Home to Work\")\n",
    "pylab.legend(loc='best')\n",
    "pylab.xlabel(\"Minutes Until Pickup\")\n",
    "pylab.ylabel(\"Number of Buses\")\n",
    "pylab.plot()\n",
    "pylab.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/usr/local/lib/python2.7/site-packages')\n",
    "\n",
    "import json\n",
    "from serializers import BusPrediction\n",
    "import pylab\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/Users/lorenamesa/Desktop/pytennessee/bus_non-labeled_data.csv\", index_col = [0, 1])\n",
    "# 461 Eastbound #72 records\n",
    "print df[(df['delayed'] == 1) & (df['stop_name'] == 'North Ave & California')]\n",
    "print df[(df['delayed'] == 0) & (df['stop_name'] == 'North Ave & California')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/usr/local/lib/python2.7/site-packages')\n",
    "\n",
    "import json\n",
    "from serializers import Tweet\n",
    "import pylab\n",
    "import pandas as pd\n",
    "import re\n",
    "import operator\n",
    "\n",
    "df = pd.read_csv(\"/Users/lorenamesa/Desktop/pytennessee/cta_tweet_data.csv\", index_col = [0, 1])\n",
    "all_tweets = {}\n",
    "all_ids = set([])\n",
    "\n",
    "def ngrams(input, n):\n",
    "    input = input.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(input)-n+1):\n",
    "        output.append(input[i:i+n])\n",
    "\n",
    "    return output\n",
    "\n",
    "# print df\n",
    "\n",
    "for indx, row in df.iterrows():\n",
    "    if pd.notnull(row[-1]) and pd.notnull(row[0]):\n",
    "        row_dict = row.to_dict()\n",
    "        print row_dict.keys()\n",
    "        clean_d = { k.replace(\"'\",\"\").strip(): v for k, v in row_dict.iteritems() }\n",
    "        break\n",
    "        tweet = Tweet(**clean_d)\n",
    "        if not all_tweets.get(tweet.tweet_id):\n",
    "            all_tweets[tweet.tweet_id] = tweet\n",
    "        else:\n",
    "            all_tweets[tweet.tweet_id] = tweet\n",
    "\n",
    "print all_tweets.keys()\n",
    "# all_tweet_bigrams = [ngrams(re.sub(r'[^\\w\\s]', '', data.text), 2) for tweet_id, data in all_tweets.iteritems()]\n",
    "# all_tweet_bigrams = sum(all_tweet_bigrams, [])\n",
    "\n",
    "# bigrams_freq = {}\n",
    "\n",
    "# for bigram in all_tweet_bigrams:\n",
    "#     joined_bigram = ' '.join(bigram)\n",
    "#     if joined_bigram not in bigrams_freq:\n",
    "#         bigrams_freq[joined_bigram] = 1\n",
    "#     else:\n",
    "#         bigrams_freq[joined_bigram] += 1\n",
    "\n",
    "\n",
    "\n",
    "# sorted_bigrams_freq = sorted(bigrams_freq.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "# # print sorted_bigrams_freq[:10] # [('trains are', 112), ('Line trains', 100), ('are operating', 70), ('residual delays', 67), ('delays after', 67), ('operating with', 67), ('with residual', 65), ('due to', 60), ('buses are', 50), ('rerouted via', 50)]\n",
    "\n",
    "# labels = [bigram[0] for bigram in sorted_bigrams_freq[:10]]\n",
    "# data = [bigram[1] for bigram in sorted_bigrams_freq[:10]]\n",
    "# pylab.title('Top 10 Bigrams in CTA Tweet Data January 2016')\n",
    "# pylab.pie(data, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "# pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n",
      "testing on 90/10 split\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.33      0.14      0.20         7\n",
      "          2       0.55      0.75      0.63         8\n",
      "          3       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.42      0.44      0.40        16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/usr/local/lib/python2.7/site-packages')\n",
    "\n",
    "import json\n",
    "from serializers import Tweet\n",
    "# import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "raw_routes_data = []\n",
    "all_labels = []\n",
    "\n",
    "# df = pd.read_csv(\"/Users/lorenamesa/Desktop/pytennessee/final_morning_training_data.csv\", index_col = [0, 1])\n",
    "# for indx, row in df.iterrows():\n",
    "#     data = row.to_dict()\n",
    "#     clean_d = { k.replace(\"'\",\"\").strip(): v for k, v in data.iteritems() }\n",
    "#     all_labels.append(clean_d.pop('prediction'))\n",
    "#     raw_routes_data.append(raw_route_data)\n",
    "\n",
    "headers = [\"utc_timestamp\", \"date_string\", \"prediction\", \"california_train\", \"weather\", \"feels_like\", \"tweet\", \"damen_train\", \"uber_surging\", \"uber_eta\", \"72_bus\", \"52_bus\"]\n",
    "\n",
    "with open(\"/Users/lorenamesa/Desktop/pytennessee/final_morning_training_data.csv\", \"r\") as morning_data:\n",
    "\n",
    "    reader = csv.reader(morning_data, dialect='excel')\n",
    "\n",
    "    for row in reader:\n",
    "        if row[0] == 'utc_timestamp':\n",
    "            continue\n",
    "        data = dict(zip(headers, row))\n",
    "        all_labels.append(data.pop('prediction'))\n",
    "        raw_route_data = dict(zip(headers, row))\n",
    "        raw_route_data.pop('prediction')\n",
    "        raw_route_data.pop('date_string')\n",
    "        raw_route_data.pop('utc_timestamp')\n",
    "        raw_routes_data.append(raw_route_data)\n",
    "\n",
    "# Apply vectorizer to training data\n",
    "print len(raw_routes_data)\n",
    "ten_percent = len(raw_routes_data) / 10\n",
    "vec = DictVectorizer()\n",
    "training_raw_data = raw_routes_data[ten_percent:]\n",
    "training_label = all_labels[ten_percent:]\n",
    "training_data = vec.fit_transform(training_raw_data).toarray()\n",
    "\n",
    "# Train classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(training_data, training_label)\n",
    "\n",
    "# Predict\n",
    "testing_label = all_labels[:ten_percent]\n",
    "testing_raw_data = raw_routes_data[:ten_percent]\n",
    "testing_data = vec.transform(testing_raw_data).toarray()\n",
    "\n",
    "predictions = clf.predict(testing_data)\n",
    "\n",
    "# Generate report for 90/10 split!\n",
    "m = metrics.classification_report(testing_label, predictions)  \n",
    "print 'testing on 90/10 split'\n",
    "print m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 to 5.16666666667\n",
      "5.16666666667 to 10.3333333333\n",
      "10.3333333333 to 15.5\n",
      "15.5 to 20.6666666667\n",
      "20.6666666667 to 25.8333333333\n",
      "25.8333333333 to 31.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/usr/local/lib/python2.7/site-packages')\n",
    "\n",
    "import json\n",
    "from serializers import BusPrediction\n",
    "import pylab\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/Users/lorenamesa/Desktop/pytennessee/bus_data.csv\", index_col = [0, 1])\n",
    "# print type(df)\n",
    "\n",
    "north_ave = [row.to_dict() for index, row in df.iterrows() if row[-1] == 'North Ave & California']\n",
    "for indx, bus in enumerate(north_ave):\n",
    "    clean_d = { k.replace(\"'\",\"\").strip(): v for k, v in bus.iteritems() }\n",
    "    north_ave[indx] = BusPrediction(**clean_d)\n",
    "    \n",
    "morning_north_ave = [float(prediction.bus_eta) for prediction in north_ave if 6 < prediction.get_requested_time_hour() < 12]\n",
    "\n",
    "pylab.figure()\n",
    "data = pylab.hist(morning_north_ave, bins=6)\n",
    "\n",
    "ranges = list(data[1][0:7])\n",
    "categories = []  # ['1.0 to 4.5', '4.5 to 8.0', '8.0 to 11.5', '11.5 to 15.0', '15.0 to 18.5', '18.5 to 22.0']\n",
    "num_categories = []\n",
    "\n",
    "for num in xrange(len(ranges)-1):\n",
    "    categories.append(str(ranges[num]) + \" to \" + str(ranges[num+1]))\n",
    "    \n",
    "for category in categories:\n",
    "    print category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/usr/local/lib/python2.7/site-packages')\n",
    "\n",
    "import json\n",
    "# import pandas as pd\n",
    "import csv\n",
    "\n",
    "\n",
    "class WorkCommutePrediction(object):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.labeled_route = kwargs.get('prediction')\n",
    "        self.utc_timestamp = kwargs.get('utc_timestamp')\n",
    "        self.local_date_string = kwargs.get('date_string')\n",
    "        self.weather = kwargs.get('weather')\n",
    "        self.feels_like = kwargs.get('feels_like')\n",
    "        self.cta_delayed = kwargs.get('tweet')\n",
    "        self.north_ave_bus_eta = kwargs.get('72_bus')\n",
    "        self.cali_bus_eta = kwargs.get('52_bus')\n",
    "        self.damen_train_eta = kwargs.get('damen_train')\n",
    "        self.cali_train_eta = kwargs.get('california_train')\n",
    "        self.uberX_eta = kwargs.get('uber_eta')\n",
    "        self.uberX_surging = kwargs.get('uber_surging')\n",
    "\n",
    "all_routes = {}\n",
    "\n",
    "with open(\"/Users/lorenamesa/Desktop/pytennessee/final_morning_training_data.csv\", \"r\") as morning_data:\n",
    "\n",
    "    reader = csv.reader(morning_data, dialect='excel')\n",
    "\n",
    "    for row in reader:\n",
    "        if row[0] == 'utc_timestamp':\n",
    "            continue\n",
    "        data = dict(zip(headers, row))\n",
    "        work_commute_p = WorkCommutePrediction(**data)\n",
    "        all_labels.append(data.pop('prediction'))\n",
    "\n",
    "        if all_routes.get(work_commute_p.labeled_route):\n",
    "            all_routes[work_commute_p.labeled_route].append(work_commute_p)\n",
    "        else:\n",
    "            all_routes[work_commute_p.labeled_route] = [work_commute_p]\n",
    "\n",
    "route_data = []\n",
    "routes = []\n",
    "\n",
    "north_bus_breakdown = {}\n",
    "for route, data in all_routes.iteritems():\n",
    "    north_bus_breakdown[route] = {}\n",
    "    for item in data:\n",
    "        if north_bus_breakdown[route].get(item.north_ave_bus_eta):\n",
    "            north_bus_breakdown[route][item.north_ave_bus_eta] += 1\n",
    "        else:\n",
    "            north_bus_breakdown[route][item.north_ave_bus_eta] = 1\n",
    "\n",
    "north_eta = ['1.0 to 6.0', '6.0 to 11.0', '11.0 to 16.0', '16.0 to 21.0', '21.0 to 26.0', '26.0 to 31.0']\n",
    "\n",
    "pylab.figure()\n",
    "pylab.title('Number of Rides Per Route By North Ave (#72) Bus ETA')\n",
    "b1 = pylab.bar([1,2,3,4,5,6], [north_bus_breakdown['1'].get(item, 0) for item in north_eta], color='red', width=0.3)\n",
    "b2 = pylab.bar([1,2,3,4,5,6], [north_bus_breakdown['2'].get(item, 0) for item in north_eta], color='orange', width=0.2)\n",
    "b3 = pylab.bar([1,2,3,4,5,6], [north_bus_breakdown['3'].get(item, 0) for item in north_eta], color='blue', width=0.1)\n",
    "\n",
    "pylab.legend([b1[0], b2[0], b3[0]], ['52 Bus -> Cali Train', '72 Bus -> Damen', 'UberX'])\n",
    "pylab.xticks([1,2,3,4,5,6], north_eta)\n",
    "\n",
    "\n",
    "pylab.xlabel('North Ave Bus ETA in Minutes')\n",
    "pylab.ylabel('Number of Commutes')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
